Backpropogation on MNIST. Trained Weights MSE = 21.6573282978
    
    - Total Data = 70,000 points
    - Train/Test Split =0.7/0.3
    - Randomly initialized weight and biases between 0 and 1
    - Learning Rate = 0.3
    - 3 Layers, 1 hidden layer: [784,9,1]
    - Sigmoid Activation Function
    - Quadratic Cost Function
    - Random weights also create an MSE of around 20 so this model doesn't do anything
